{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Farhan: This is a copy of the file from P23-18 Transitiemanager we discussed in 19/08/2024. You can used snippets from it to create a workbook for the creation of a zoning layer for Hasselt (or convert this one into the workbook for Hasselt).\n",
    "\n",
    "# Extract buildings from OSM\n",
    "\n",
    "Install required libraries using the Conda conda-forge channel\n",
    "````bash\n",
    "conda install -c conda-forge jupyter\n",
    "conda install -c conda-forge requests\n",
    "````\n",
    "\n",
    "Download relevant files to `./data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)  # create data folder if not exist\n",
    "\n",
    "\n",
    "# Download file\n",
    "def dlfile(furl, fname):\n",
    "    if not os.path.isfile(fname):\n",
    "        print(f\"File {fname} does not exist.\")\n",
    "        print(f\"Downloading from {furl}\")\n",
    "\n",
    "        chunk_size = 4096\n",
    "        with requests.get(furl, stream=True) as r:\n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "    else:\n",
    "        print(f\"File {fname} is available.\")\n",
    "\n",
    "\n",
    "print(\"Acquiring required data\")\n",
    "\n",
    "# Statistische sectoren\n",
    "SSUrl = \"https://statbel.fgov.be/sites/default/files/files/opendata/Statistische%20sectoren/sh_statbel_statistical_sectors_3812_20230101.shp.zip\"\n",
    "SSFile = \"./data/sh_statbel_statistical_sectors_3812_20230101.shp.zip\"\n",
    "dlfile(SSUrl, SSFile)\n",
    "\n",
    "# Bevolking per statistische sector\n",
    "BVSSUrl = \"https://statbel.fgov.be/sites/default/files/files/opendata/bevolking/sectoren/OPEN%20DATA_SECTOREN_2019.xlsx\"\n",
    "BVSSFile = \"./data/OPEN_DATA_SECTOREN_2019.xlsx\"\n",
    "dlfile(BVSSUrl, BVSSFile)\n",
    "\n",
    "# OSM data voor BelgiÃ«\n",
    "OSMUrl = \"https://download.geofabrik.de/europe/belgium-latest.osm.pbf\"\n",
    "OSMFile = \"./data/belgium.osm.pbf\"\n",
    "dlfile(OSMUrl, OSMFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optioneel/test) Cut de oorspronkelijke pbf bestanden in kleinere\n",
    "Gebruik Osmium.\n",
    "\n",
    "````bash\n",
    "pip install osmium\n",
    "````\n",
    "\n",
    "Cut obv boudning box.\n",
    "Bounding box tool: https://boundingbox.klokantech.com/ (selecteer CSV).\n",
    "\n",
    "- AVBL (Antwerpen, Vlaams-Brabant, Limburg): 3.85,50.7121,6.1183,51.5039\n",
    "- Hasselt: 5.265463,50.894442,5.452711,50.956291\n",
    "\n",
    "````bash\n",
    "# AVBL\n",
    "osmium extract --bbox 3.85,50.7121,6.1183,51.5039 ./data/belgium.osm.pbf -o ./data/AVBL.osm.pbf\n",
    "# Hasselt\n",
    "osmium extract --bbox 5.265463,50.894442,5.452711,50.956291 ./data/belgium.osm.pbf -o ./data/Hasselt.osm.pbf\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gebruik voor tests het beperkte bestand\n",
    "!osmium extract --overwrite --bbox 5.265463,50.894442,5.452711,50.956291 ./data/belgium.osm.pbf -o ./data/Hasselt.osm.pbf\n",
    "OSMFile = \"./data/Hasselt.osm.pbf\" \n",
    "OSMUrl = \"\"\n",
    "\n",
    "!osmium extract --overwrite --bbox 3.85,50.7121,6.1183,51.5039 ./data/belgium.osm.pbf -o ./data/AVBL.osm.pbf\n",
    "OSMFile = \"./data/AVBL.osm.pbf\" \n",
    "OSMUrl = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create shp files with populatie\n",
    "\n",
    "Use official data from the statistical center and start from the SS to create files for DLG and GEM level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# gpd.options.io_engine = \"pyogrio\"\n",
    "# import fiona\n",
    "\n",
    "# https://geopandas.org/en/v0.14.0/docs/user_guide/io.html\n",
    "#     where=\"CNIS_PROVI='70000'\",\n",
    "#     where=\"(CNIS_PROVI IN ('70000', '20001', '10000')) OR ( CNIS_PROVI IS NULL)\",\n",
    "gdf_SS = gpd.read_file(\n",
    "    f\"zip://{SSFile}!sh_statbel_statistical_sectors_3812_20230101.shp\",\n",
    "    where=\"(CNIS_PROVI IN ('70000', '20001', '10000')) OR ( CNIS_PROVI IS NULL)\",\n",
    "    columns=[\n",
    "        \"CS01012023\",\n",
    "        \"CNIS_PROVI\",\n",
    "        \"T_SEC_NL\",\n",
    "        \"CNIS_REGIO\",\n",
    "        \"T_ARRD_NL\",\n",
    "        \"T_MUN_NL\",\n",
    "        \"CNIS5_2023\",\n",
    "        \"C_NIS6\",\n",
    "        \"T_NIS6_NL\",\n",
    "    ],\n",
    ")\n",
    "# Pyogrio: https://pyogrio.readthedocs.io/en/latest/introduction.html#read-a-subset-of-columns\n",
    "# gdf = gdf[[\"CS01012023\",\n",
    "#        \"CNIS_PROVI\",\n",
    "#        \"T_SEC_NL\",\n",
    "#        \"CNIS_REGIO\",\n",
    "#        \"T_ARRD_NL\",\n",
    "#        \"T_MUN_NL\", \"geometry\"\n",
    "#    ]]\n",
    "print(gdf_SS.columns)\n",
    "gdf_SS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load excel file with population per SS in Pandas dataframe\n",
    "\n",
    "````bash\n",
    "conda install -c conda-forge openpyxl\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# bvss = pd.read_excel(BVSSFile)\n",
    "bvss = pd.read_excel(BVSSFile, usecols=[\"POPULATION\", \"CD_SECTOR\"])\n",
    "bvss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join bvss dataframe with population with geodataframe with SS data to add population data per SS to the SS shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_SS = gdf_SS.merge(bvss, left_on=\"CS01012023\", right_on=\"CD_SECTOR\")\n",
    "gdf_SS.head()\n",
    "# drop unneeded foreign key column before export\n",
    "gdf_SS = gdf_SS.drop([\"CD_SECTOR\"], axis=\"columns\")\n",
    "gdf_SS.to_file(\"./data/SS_pop.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create shapefiles for DLG and GEM by aggregation of SS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate SS to DLG\n",
    "gdf_DLG = gdf_SS.dissolve(\n",
    "    by=\"C_NIS6\",  # Do not aggregate on T_NIS6_NL as it cannot be an index: duplicate names!\n",
    "    aggfunc={  # Only listed columns are exported/aggregated\n",
    "        \"CNIS_PROVI\": \"first\",\n",
    "        \"CNIS_REGIO\": \"first\",\n",
    "        \"T_ARRD_NL\": \"first\",\n",
    "        \"T_MUN_NL\": \"first\",\n",
    "        \"T_NIS6_NL\": \"first\",\n",
    "        \"CNIS5_2023\": \"first\",\n",
    "        \"POPULATION\": \"sum\",\n",
    "    },\n",
    ").reset_index()  # reset newly created C_NIS6 index to a regular column\n",
    "gdf_DLG.to_file(\"./data/DLG_pop.shp\")\n",
    "\n",
    "# Aggregate DLG to GEM\n",
    "gdf_GEM = gdf_DLG.dissolve(\n",
    "    by=\"CNIS5_2023\",  # Do not aggregate on T_MUN_NL as it is a label and not necessarily unique\n",
    "    aggfunc={  # Only listed columns are exported/aggregated\n",
    "        \"CNIS_PROVI\": \"first\",\n",
    "        \"CNIS_REGIO\": \"first\",\n",
    "        \"T_ARRD_NL\": \"first\",\n",
    "        \"POPULATION\": \"sum\",\n",
    "        \"T_MUN_NL\": \"first\",\n",
    "    },\n",
    ").reset_index()  # reset newly created CNIS5_2023 index to a regular column\n",
    "gdf_GEM.to_file(\"./data/GEM_pop.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SS per DLG to obtain a x_pct % of the population\n",
    "\n",
    "Use pivot_table on gdf with the SS and the population. We count how many SS, ordered from large to low population are needed to cover x_pct % of the population in that DLG.\n",
    "\n",
    "This value is used later as the minimal amount of clusters to create for that DLG.\n",
    "\n",
    "https://www.codium.ai/blog/pandas-pivot-tables-a-comprehensive-guide-for-data-science/\n",
    "\n",
    "````bash\n",
    "mamba install -c conda-forge geopy\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrema:\n",
    "# x_pct = 0   -> No min required population represented in selected SS => 1 cluster for DLG\n",
    "# x_pct = 100 -> Full population needs to be represented in selected SS => # clusters = # SS in DLG\n",
    "#                Note: # clusters can be < # SS in DLG if there are SS with POPULATION = 0\n",
    "x_pct = 66\n",
    "\n",
    "import numpy as np\n",
    "from shapely.ops import unary_union, nearest_points\n",
    "\n",
    "from geopy.geocoders import Photon\n",
    "import pyproj\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "\n",
    "wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "\n",
    "photon_url = \"imobwww.uhasselt.be/photon\"\n",
    "geolocator = Photon(user_agent=\"Python\", domain=photon_url)\n",
    "location = geolocator.geocode(\"Dusart Hasselt\")\n",
    "print(location.address)\n",
    "print((location.latitude, location.longitude))\n",
    "project = pyproj.Transformer.from_crs(wgs84, gdf_SS.crs, always_xy=True).transform\n",
    "dusart = Point(location.longitude, location.latitude)\n",
    "print(dusart)\n",
    "dusart = transform(project, Point(location.longitude, location.latitude))\n",
    "print(dusart)\n",
    "\n",
    "location2 = geolocator.geocode(\"begijnenstraat diest\")\n",
    "print(location2.address)\n",
    "print((location2.latitude, location2.longitude))\n",
    "# project = pyproj.Transformer.from_crs(wgs84, gdf_SS.crs, always_xy=True).transform\n",
    "diepenbeek = Point(location2.longitude, location2.latitude)\n",
    "print(diepenbeek)\n",
    "diepenbeek = transform(project, Point(location2.longitude, location2.latitude))\n",
    "print(diepenbeek)\n",
    "\n",
    "print(diepenbeek.distance(dusart))\n",
    "\n",
    "\n",
    "# Get the number of the largest SS required to obtain x_pct of the POPULATION\n",
    "# -np.sort(-x) sorts the series descending\n",
    "# np.cumsum(-np.sort(-x)) generates total numbers of POPULATION by incrementally adding largest SS\n",
    "# np.sum(x) = total inhabitants in DLG\n",
    "# x_pct*np.sum(x)/100 : number of inhabitants in DLG that consitutes x_pct of total in DLG\n",
    "# np.cumsum(-np.sort(-x))-x_pct*np.sum(x)/100 : becomes 0 for the n largest SS that cover x_pct of pop in DLG\n",
    "# np.abs(...) becomes 0 for the element in the series for which the cumsum becomes x_pct of pop in DLG\n",
    "# np.argmin( ...) computes the 0-based index of the value closest by 0\n",
    "# ... + 1 : convert 0-based index to number of SS\n",
    "def xpct(x):\n",
    "    return np.argmin(np.abs(np.cumsum(-np.sort(-x)) - x_pct * np.sum(x) / 100)) + 1\n",
    "\n",
    "\n",
    "def mindist(x):\n",
    "    return dusart.distance(unary_union(x)) / 1000\n",
    "\n",
    "\n",
    "# def mindist2(x): return dusart.distance(unary_union(x))\n",
    "# def mindist(x): return min(dusart.distance(x))\n",
    "\n",
    "# first test: compute the mean population ...\n",
    "DLG_pivot_gdf = gdf_SS.pivot_table(\n",
    "    index=\"C_NIS6\",\n",
    "    values=[\"POPULATION\", \"geometry\", \"T_NIS6_NL\"],\n",
    "    aggfunc={\n",
    "        \"POPULATION\": [\"count\", xpct],\n",
    "        \"geometry\": [mindist],\n",
    "        \"T_NIS6_NL\": \"first\",\n",
    "    },\n",
    ")\n",
    "DLG_pivot_gdf = DLG_pivot_gdf.rename(\n",
    "    columns={\"xpct\": f\"n_pct_{x_pct}\", \"first\": \"T_NIS6_NL\"}\n",
    ")\n",
    "# Flatten multi level column index\n",
    "DLG_pivot_gdf.columns = [a[-1] for a in DLG_pivot_gdf.columns.to_flat_index()]\n",
    "# Reset Index to get the Name column back\n",
    "DLG_pivot_gdf = DLG_pivot_gdf.reset_index()\n",
    "print(DLG_pivot_gdf)\n",
    "print(\n",
    "    DLG_pivot_gdf.loc[\n",
    "        DLG_pivot_gdf[\"T_NIS6_NL\"].isin([\"DIEPENBEEK\", \"HASSELT\", \"DIEST\"])\n",
    "    ]\n",
    ")\n",
    "print(gdf_SS.crs)\n",
    "\n",
    "# Plot histogram with the distances\n",
    "ax = DLG_pivot_gdf.plot.hist(column=\"mindist\")\n",
    "ax.set_title(\"Distance from Dustart to DLGs\")\n",
    "ax.set_xlabel(\"Distance (Kilometer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract building data from pbf file\n",
    "\n",
    "Maak een polygoon voor Limburg om mee te filteren: LimburgGrenzen.\n",
    "Lees de gebouwen in.\n",
    "Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First try: Pyrosm. Does not scale well (memory consumption)\n",
    "# from pyrosm import OSM, get_data\n",
    "#\n",
    "# osm = OSM(OSMFile)\n",
    "# buildings = osm.get_buildings()\n",
    "# buildings.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Osmium to process OSM pbf file\n",
    "\n",
    "````bash\n",
    "conda install -c conda-forge osmium-tool\n",
    "# Besides osmium-tool, osmium itself needs to be installed as well\n",
    "pip install osmium\n",
    "````\n",
    "https://max-coding.medium.com/extracting-open-street-map-osm-street-data-from-data-files-using-pyosmium-afca6eaa5d00\n",
    "\n",
    "https://github.com/osmcode/pyosmium/blob/master/examples/amenity_list.py\n",
    "\n",
    "https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code counting the nodes in the OSMFile pbf file\n",
    "\n",
    "import osmium\n",
    "\n",
    "\n",
    "class CounterHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        osmium.SimpleHandler.__init__(self)\n",
    "        self.num_nodes = 0\n",
    "\n",
    "    def node(self, n):\n",
    "        self.num_nodes += 1\n",
    "\n",
    "\n",
    "h = CounterHandler()\n",
    "h.apply_file(OSMFile)\n",
    "print(\"Number of nodes: %d\" % h.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract buildings from OSMFile pbf file\n",
    "\n",
    "import osmium\n",
    "import shapely.wkt as wktlib\n",
    "import geopandas\n",
    "\n",
    "# A global factory that creates WKB from a osmium geometry\n",
    "wktfab = osmium.geom.WKTFactory()\n",
    "\n",
    "# create transformer object for Shapely object crs transformation from\n",
    "# .. -> ... needed for the area computation\n",
    "# see docs: https://shapely.readthedocs.io/en/latest/manual.html#shapely.ops.transform\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "\n",
    "ss_crs = gdf_SS.crs  # use the orthonormal projection system from the SS layer\n",
    "project = pyproj.Transformer.from_crs(\n",
    "    pyproj.CRS(\"EPSG:4326\"), ss_crs, always_xy=True\n",
    ").transform\n",
    "\n",
    "\n",
    "class CounterHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        osmium.SimpleHandler.__init__(self)\n",
    "        self.num_nodes = 0\n",
    "        self.building_values = []\n",
    "        self.amenity_values = []\n",
    "        self.buildings = []\n",
    "        self.building_area = []\n",
    "\n",
    "    def area(self, n):\n",
    "        if \"building\" in n.tags:\n",
    "            try:\n",
    "                wkt = wktfab.create_multipolygon(n)\n",
    "                self.num_nodes += 1\n",
    "                area = wktlib.loads(wkt)\n",
    "                # print(n.tags[\"building\"])\n",
    "                self.buildings.append(area.centroid)\n",
    "                self.building_values.append(n.tags.get(\"building\"))\n",
    "                #                self.building_values.update([k for k, v in n.tags])\n",
    "                #                if \"amenity\" in n.tags:\n",
    "                self.amenity_values.append(n.tags.get(\"amenity\"))\n",
    "                self.building_area.append(\n",
    "                    transform(project, area).area\n",
    "                )  # transform before computing area (inefficient!)\n",
    "                if self.num_nodes % 250000 == 0:  # report one in 250000 buildings\n",
    "                    print(f\"{area.centroid} : {n.tags}\")\n",
    "                    print(f\"Building: {n.tags.get('building')}\")\n",
    "                    print(f\"Amenity: {n.tags.get('amenity')}\")\n",
    "            except:\n",
    "                print(\"Trouble with polygons ...\")\n",
    "\n",
    "\n",
    "h = CounterHandler()\n",
    "h.apply_file(OSMFile, locations=True)\n",
    "print(\"Number of nodes: %d\" % len(h.buildings))\n",
    "print(f\"Buildings: {h.building_values}\")\n",
    "print(f\"Amenities: {h.amenity_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write some statistics on the imported data\n",
    "\n",
    "https://www.tutorialspoint.com/how-to-count-occurrences-of-specific-value-in-pandas-column \n",
    "\n",
    "This does not work for Python series directly, rather, we first convert to a Pandas Series object and use the values_counts() method.\n",
    "\n",
    "Note: Alternative is to call the values_counts method later on the columns of the blding_centroids GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "building_value_counts = pd.Series(h.building_values).value_counts()\n",
    "amenity_value_counts = pd.Series(h.amenity_values).value_counts()\n",
    "\n",
    "print(f\"Buildings values: {building_value_counts}\")\n",
    "print(f\"Amenities values: {amenity_value_counts}\")\n",
    "\n",
    "# write results to excel file\n",
    "with pd.ExcelWriter(\"./data/tag_stats.xlsx\") as writer:\n",
    "    building_value_counts.to_excel(writer, \"building\")\n",
    "    amenity_value_counts.to_excel(writer, \"amenity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write shapefile with centroids to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "print(h.building_values)\n",
    "\n",
    "# bldng_centroids = gpd.GeoDataFrame( geometry=h.buildings, crs=\"EPSG:4326\")\n",
    "# bldng_centroids = gpd.GeoDataFrame({'building': h.building_values, 'amenity': h.amenity_values, 'geometry': gpd.GeoSeries(h.buildings)}, crs=\"EPSG:4326\")\n",
    "bldng_centroids = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"building\": h.building_values,\n",
    "        \"amenity\": h.amenity_values,\n",
    "        \"area\": h.building_area,\n",
    "        \"geometry\": h.buildings,\n",
    "    },\n",
    "    geometry=\"geometry\",\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "bldng_centroids = bldng_centroids.to_crs(gdf_SS.crs)  # reproject to SS shapefile crs\n",
    "\n",
    "bldng_centroids.to_file(\"./data/bldng_centroids.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the OSM points in each of the statistical sectors\n",
    "Based on the following example: https://medium.com/@nygeog/data-science-methods-focus-geoprocessing-with-geopandas-using-spatial-joins-counting-points-e42d1b36d758\n",
    "\n",
    "Note: this merely counts the number of points. If we can have points belonging to different categories (e.g. for different amenities), we can use the Pandas pivot table approach as in: https://gis.stackexchange.com/questions/306674/geopandas-spatial-join-and-count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join dataframes\n",
    "joined_gdf = gdf_SS.sjoin(\n",
    "    bldng_centroids,\n",
    "    how=\"inner\",\n",
    "    predicate=\"intersects\",\n",
    ")\n",
    "# Groepeer op SS en tel\n",
    "ss_count_bldngs = joined_gdf.groupby(\n",
    "    [\"CS01012023\"],\n",
    "    as_index=False,\n",
    ")[\n",
    "    \"building\"\n",
    "].count()  # this counts the non Null values, so choose column that has no Null\n",
    "ss_count_bldngs = ss_count_bldngs.rename(columns={\"building\": \"bldng_cnt\"})\n",
    "# ss_count_bldngs.columns = ['building', 'bldng_cnt']  # column rename alternative\n",
    "# print(ss_count_bldngs.head())\n",
    "\n",
    "# Merge dataframe with counts per SS back into the dataframe with the SS\n",
    "# print(gdf.head())\n",
    "gdf_cnts = gdf_SS.merge(\n",
    "    ss_count_bldngs,\n",
    "    on=\"CS01012023\",\n",
    "    how=\"left\",\n",
    ")\n",
    "# print(gdf_cnts.head())\n",
    "\n",
    "# Cleanup\n",
    "# Note: all SS outside the SS got NaN since no matching value was found in the count df\n",
    "# Also, SS without buildings will be missing; set NaN -> 0\n",
    "gdf_cnts[\"bldng_cnt\"].fillna(\n",
    "    0,\n",
    "    inplace=True,\n",
    ")\n",
    "# NaN has no integer representation, so the bldng_cnt column is float\n",
    "# As they are counts, convert to int\n",
    "gdf_cnts = gdf_cnts.astype({\"bldng_cnt\": \"int\"})\n",
    "\n",
    "# print(gdf_cnts.head())\n",
    "gdf_cnts.to_file(\"./data/SS_pop_bldng_cnts.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###import matplotlib.pyplot as plt\n",
    "###%matplotlib inline\n",
    "###\n",
    "###gdf_cnts.plot(\n",
    "###    column='bldng_cnt',\n",
    "###    figsize=(8, 5),\n",
    "###    cmap='magma',\n",
    "###    legend=True,\n",
    "###)\n",
    "###plt.title('Number of OSM buildings per Statistical Sector');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import folium\n",
    "###\n",
    "### # See: https://vverde.github.io/blob/interactivechoropleth.html\n",
    "###\n",
    "### #map = folium.Map(location=[50.93, 5.36], tiles=\"OpenStreetMap\", zoom_start=13)\n",
    "### #map\n",
    "###\n",
    "### # Compute center of data\n",
    "### gdf_cnts = gdf_cnts.to_crs('EPSG:4326')\n",
    "### x_map=gdf_cnts.centroid.x.mean()\n",
    "### y_map=gdf_cnts.centroid.y.mean()\n",
    "### print(x_map,y_map)\n",
    "###\n",
    "### mymap = folium.Map(location=[y_map, x_map], zoom_start=11, tiles=None)\n",
    "### folium.TileLayer('CartoDB positron',name=\"Light Map\",control=False).add_to(mymap)\n",
    "### #myscale = (gdf_cnts['bldng_cnt'].quantile((0,0.25,0.5,0.75,1))).tolist()\n",
    "### #print(myscale)\n",
    "### folium.Choropleth(\n",
    "###     geo_data=gdf_cnts,\n",
    "###     name='Choropleth',\n",
    "###     data=gdf_cnts,\n",
    "###     columns=['CS01012023','bldng_cnt'],\n",
    "###     key_on=\"feature.properties.CS01012023\",\n",
    "###     fill_color='YlGn',\n",
    "###     scheme='Quantiles',\n",
    "### #    threshold_scale=myscale,\n",
    "###     fill_opacity=0.5,\n",
    "###     line_opacity=0.2,\n",
    "###     legend_name='Number of buildings in Statistical Sector',\n",
    "###     smooth_factor=0\n",
    "### ).add_to(mymap)\n",
    "### mymap\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # alternative choropleth map\n",
    "### # See: https://python-visualization.github.io/folium/latest/advanced_guide/colormaps.html for color options\n",
    "### import branca.colormap as cm\n",
    "### colormap = cm.linear.RdPu_04.to_step(data=gdf_cnts['bldng_cnt'], n=5, method='linear')\n",
    "### colormap\n",
    "###\n",
    "### # Compute center of data\n",
    "### gdf_cnts = gdf_cnts.to_crs('EPSG:4326')\n",
    "### x_map=gdf_cnts.centroid.x.mean()\n",
    "### y_map=gdf_cnts.centroid.y.mean()\n",
    "### print(x_map,y_map)\n",
    "###\n",
    "### mymap = folium.Map(location=[y_map, x_map], zoom_start=11, tiles=None)\n",
    "### folium.TileLayer('CartoDB positron',name=\"Light Map\",control=False).add_to(mymap)\n",
    "###\n",
    "### colormap.caption = \"Number of buildings in Statistical Sector\"\n",
    "### style_function = lambda x: {\"weight\":0.5,\n",
    "###                             'color':'black',\n",
    "###                             'fillColor':colormap(x['properties']['bldng_cnt']),\n",
    "###                             'fillOpacity':0.75}\n",
    "### highlight_function = lambda x: {'fillColor': '#000000',\n",
    "###                                 'color':'#000000',\n",
    "###                                 'fillOpacity': 0.50,\n",
    "###                                 'weight': 0.1}\n",
    "### NIL=folium.features.GeoJson(\n",
    "###         gdf_cnts,\n",
    "###         style_function=style_function,\n",
    "###         control=False,\n",
    "###         highlight_function=highlight_function,\n",
    "###         tooltip=folium.features.GeoJsonTooltip(fields=['CS01012023','bldng_cnt','POPULATION'],\n",
    "###             aliases=['SSID','# buildings','Population'],\n",
    "###             style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\"),\n",
    "###             sticky=True\n",
    "###         )\n",
    "###     )\n",
    "### colormap.add_to(mymap)\n",
    "### mymap.add_child(NIL)\n",
    "### mymap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering van de gebouwen in groepen\n",
    "\n",
    "https://samdotson1992.github.io/SuperGIS/blog/k-means-clustering/\n",
    "https://darribas.org/gds15/content/labs/lab_08.html\n",
    "\n",
    "https://gis.stackexchange.com/questions/309087/how-to-create-a-geodataframe-points-grid-from-a-numpy-mgrid\n",
    "\n",
    "https://stackoverflow.com/questions/50971914/what-is-the-most-efficient-way-to-convert-numpy-arrays-to-shapely-points\n",
    "\n",
    "Selecteer eerst enkel gebouwen in Halen (T_NIS6_NL=\"HALEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecteer Halen als gebied om gebouwen mee te selecteren\n",
    "# gdf_halen = gdf_SS[gdf_SS.T_MUN_NL == \"Hasselt\"].dissolve(by='T_MUN_NL')\n",
    "# Fix issues met Memory leak op Windows\n",
    "# https://stackoverflow.com/questions/69596239/how-to-avoid-memory-leak-when-dealing-with-kmeans-for-example-in-this-code-i-am\n",
    "\n",
    "# Use slugify to remove invalid characters from autogenerated paths\n",
    "# mamba install -c conda-forge python-slugify\n",
    "from slugify import slugify\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from colorama import Fore\n",
    "from colorama import Style\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import math\n",
    "\n",
    "# Store the centroids for the cluster, including their cluster ID (cluster)\n",
    "# Note that clusters with the same label for different DLG have no relation.\n",
    "gdf_ccentroids = geopandas.GeoDataFrame(\n",
    "    columns=[\"X\", \"Y\", \"cluster\", \"geometry\"], geometry=\"geometry\", crs=gdf_SS.crs\n",
    ")\n",
    "gdf_centroids = geopandas.GeoDataFrame(\n",
    "    columns=[\n",
    "        \"building\",\n",
    "        \"amenity\",\n",
    "        \"area\",\n",
    "        \"CNIS_PROVI\",\n",
    "        \"CNIS_REGIO\",\n",
    "        \"CS01012023\",\n",
    "        \"T_ARRD_NL\",\n",
    "        \"T_MUN_NL\",\n",
    "        \"T_NIS6_NL\",\n",
    "        \"T_SEC_NL\",\n",
    "        \"POPULATION\",\n",
    "        \"cluster\",\n",
    "        \"geometry\",\n",
    "    ],\n",
    "    geometry=\"geometry\",\n",
    ")\n",
    "\n",
    "cluster_offset = 0  # Offset to generate unique cluster IDs over DLG\n",
    "\n",
    "# Itereer over de deelgemeenten\n",
    "# for i_dlg in gdf_SS.sort_values(by=['T_MUN_NL', 'T_NIS6_NL'])['T_NIS6_NL'].unique():\n",
    "# Sort on the names (T attributes) but group on the codes/indices (C attributes) to deal with duplicate naming\n",
    "for group_k, group_v in gdf_SS.sort_values(by=[\"T_MUN_NL\", \"T_NIS6_NL\"]).groupby(\n",
    "    by=[\"CNIS5_2023\", \"C_NIS6\"], sort=False\n",
    "):\n",
    "    i_gem = group_k[0]  # group_k is a tuple\n",
    "    i_dlg = group_k[1]\n",
    "    i_gem_name = group_v[\"T_MUN_NL\"].iloc[\n",
    "        0\n",
    "    ]  # group_v is a Pandas Series of (identical) values for each member of the group, take the first element\n",
    "    i_dlg_name = group_v[\"T_NIS6_NL\"].iloc[0]\n",
    "    # Determine municipality (gemeente) to which the submunicipality (deelgemeente) belongs\n",
    "    # BUG : this finds multiple GEM for DLG with the same name, e.g. Halle (Halle) and Halle (Zoersel)\n",
    "    #       no major issue, but double processing\n",
    "    # i_gem = gdf_SS[gdf_SS[\"T_NIS6_NL\"] == i_dlg].iloc[0][\"T_MUN_NL\"]\n",
    "\n",
    "    # Switch for testing\n",
    "    if True:\n",
    "        #    if i_gem_name in [ 'Deurne', 'Halle', 'Diest', 'Hasselt', 'Diepenbeek']:\n",
    "        # Note: Deurne only exists as DLG, not as GEM. So it should not pop up in the list\n",
    "        print(f\"{Fore.YELLOW}Processing {i_dlg_name} ({i_gem_name}) {Style.RESET_ALL}\")\n",
    "\n",
    "        # Create folder for DLG specific data\n",
    "        os.makedirs(\n",
    "            f\"./data/{slugify(i_dlg_name)}_{slugify(i_gem_name)}\", exist_ok=True\n",
    "        )  # create data folder if not exist\n",
    "\n",
    "        # Retrieve the number of the largest SS required to cover x_pct of the i_dlg population\n",
    "        # This will be the lower bound of the number of clusters\n",
    "        # (Use iloc[0] and not [0] as there is no row index starting from 0)\n",
    "        n_x_pct = DLG_pivot_gdf.loc[\n",
    "            DLG_pivot_gdf[\"C_NIS6\"] == i_dlg, f\"n_pct_{x_pct}\"\n",
    "        ].iloc[0]\n",
    "        mindist = DLG_pivot_gdf.loc[DLG_pivot_gdf[\"C_NIS6\"] == i_dlg, \"mindist\"].iloc[0]\n",
    "\n",
    "        # Select the ss falling withing i_dlg\n",
    "        ss_i_dlg_gdf = gdf_SS[gdf_SS[\"C_NIS6\"] == i_dlg]\n",
    "\n",
    "        # Only keep building centroids falling within i_dlg\n",
    "        bldng_centroids_dlg = bldng_centroids.overlay(\n",
    "            ss_i_dlg_gdf, how=\"intersection\", make_valid=True\n",
    "        )\n",
    "        a = pd.Series(bldng_centroids_dlg[\"geometry\"].apply(lambda p: p.x))\n",
    "        b = pd.Series(bldng_centroids_dlg[\"geometry\"].apply(lambda p: p.y))\n",
    "        X = np.column_stack((a, b))\n",
    "\n",
    "        silhouette_coefficients = {}\n",
    "        n_ss = len(ss_i_dlg_gdf)  # number of statistical sectors\n",
    "        print(f\"Number of statistical sectors: {n_ss}\")\n",
    "        # We can't cluster without points (e.g. Spalbeek (Hasselt)), report and skip\n",
    "        n_bldng_dlg = len(\n",
    "            bldng_centroids_dlg\n",
    "        )  # number of buildings in DLG (= number of samples in dataset)\n",
    "        if n_bldng_dlg == 0:\n",
    "            print(f\"No points to cluster found in DLG. Moving on ...\")\n",
    "            continue\n",
    "        print(f\"Number of buildings in DLG: {n_bldng_dlg}\")\n",
    "        print(\n",
    "            f\"{n_x_pct} largest SS contain(s) at least {x_pct}% of the {i_dlg_name} population\"\n",
    "        )\n",
    "\n",
    "        # Try different number of clusters, ranging from the amount of SS needed to cover x_pct of the population of the DLG up to\n",
    "        # the number of SS in the DLG\n",
    "        if mindist < 25:\n",
    "            n_clust_min = n_x_pct  # Minimal amount of clusters for DLG\n",
    "            n_clust_max = n_ss  # Maximal amount of clusters for DLG\n",
    "        elif mindist < 50:\n",
    "            n_clust_min = math.floor(n_x_pct / 2) + 1\n",
    "            n_clust_max = math.floor(n_ss / 2) + 1\n",
    "        else:\n",
    "            n_clust_min = 1\n",
    "            n_clust_max = 1\n",
    "        if n_clust_max < n_clust_min:\n",
    "            n_clust_min = n_clust_max\n",
    "\n",
    "        # entry of iteration loop for feedback from previous clustering\n",
    "        repeat_dlg_clustering = True\n",
    "        while repeat_dlg_clustering:\n",
    "            # If we have less samples than the minimal number of clusters, reduce number of clusters\n",
    "            if n_clust_min > n_bldng_dlg:\n",
    "                n_clust_min = n_bldng_dlg\n",
    "            # If we have less samples than the maximal number of clusters, reduce number of clusters\n",
    "            if n_clust_max > n_bldng_dlg:\n",
    "                n_clust_max = n_bldng_dlg\n",
    "            for i_n_clust in range(n_clust_min, n_clust_max + 1):\n",
    "                kmeans = KMeans(\n",
    "                    n_clusters=i_n_clust,\n",
    "                    init=\"k-means++\",\n",
    "                    random_state=42,\n",
    "                    n_init=\"auto\",\n",
    "                )\n",
    "                kmeans.fit(X)\n",
    "\n",
    "                # Initialise Silhouette score to worst possible value\n",
    "                sil_coeff = -1\n",
    "                if i_n_clust > 1 and i_n_clust < n_bldng_dlg:\n",
    "                    # Compute Silhouette score if it can be computed (requires at least 2 clusters/labels)\n",
    "                    sil_coeff = silhouette_score(X, kmeans.labels_, metric=\"euclidean\")\n",
    "                elif i_n_clust > 1 and i_n_clust == n_bldng_dlg:\n",
    "                    # If the number of clusters is equal to the number of observations, we can get a perfect match\n",
    "                    # but can't compute the silhouette function. Set it to max value (1)\n",
    "                    sil_coeff = 1\n",
    "                silhouette_coefficients[i_n_clust] = sil_coeff\n",
    "\n",
    "                # Compute the classes for the centroids (which might be silly as they are probably the row index of kmeans.labels_, but can't find it in the documentation now. Better be safe.)\n",
    "                Xcentroids = np.column_stack(\n",
    "                    (kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1])\n",
    "                )\n",
    "                y_centroids = kmeans.predict(Xcentroids)\n",
    "\n",
    "                print(\n",
    "                    f\"# clusters: {i_n_clust}, Inertia: {kmeans.inertia_}, Silhouette coefficient: {sil_coeff}\"\n",
    "                )\n",
    "\n",
    "                # Create dataframe with the cluster centroids and the labels of the clusters\n",
    "                df_ccentroids_tmp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"X\": kmeans.cluster_centers_[:, 0],\n",
    "                        \"Y\": kmeans.cluster_centers_[:, 1],\n",
    "                        \"cluster\": y_centroids,\n",
    "                    }\n",
    "                )\n",
    "                # df_ccentroids_tmp.apply(lambda row: print(row), axis=1)\n",
    "                df_ccentroids_tmp[\"geometry\"] = df_ccentroids_tmp.apply(\n",
    "                    lambda row: Point(row[\"X\"], row[\"Y\"]), axis=1\n",
    "                )\n",
    "                # print(gdf_cnts.crs) # find better way to set crs\n",
    "                gdf_ccentroids_tmp = gpd.GeoDataFrame(df_ccentroids_tmp).set_crs(\n",
    "                    crs=gdf_cnts.crs\n",
    "                )\n",
    "                gdf_ccentroids_tmp.to_file(\n",
    "                    f\"./data/{slugify(i_dlg_name)}_{slugify(i_gem_name)}/clusters_{slugify(i_dlg_name)}_{slugify(i_gem_name)}_{i_n_clust}_centroids.shp\"\n",
    "                )\n",
    "\n",
    "                # Create dataframe with classes added to the building centroids\n",
    "                y_kmeans = kmeans.predict(X)\n",
    "                # Todo: bovenstaand is gelijk aan kmeans.labels_ ??\n",
    "                df_centroids_tmp = pd.DataFrame(y_kmeans, columns=[\"cluster\"])\n",
    "                df_centroids_tmp = df_centroids_tmp.assign(n_ss=n_ss)\n",
    "                gdf_centroids_tmp = bldng_centroids_dlg.join(df_centroids_tmp)\n",
    "                gdf_centroids_tmp.to_file(\n",
    "                    f\"./data/{slugify(i_dlg_name)}_{slugify(i_gem_name)}/clusters_{slugify(i_dlg_name)}_{slugify(i_gem_name)}_{i_n_clust}.shp\"\n",
    "                )\n",
    "\n",
    "            # Get the smallest number of clusters (dict key) that has the highest sihouette function value (dict value)\n",
    "            optimal_nr_clusters = max(\n",
    "                sorted(silhouette_coefficients), key=silhouette_coefficients.get\n",
    "            )\n",
    "            print(\n",
    "                f\"Optimal number of clusters: {Fore.RED}{optimal_nr_clusters}{Style.RESET_ALL}\"\n",
    "            )\n",
    "\n",
    "            # Now we have found the optimal number of clusters, rerun the clustering to test and export the data\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=optimal_nr_clusters,\n",
    "                init=\"k-means++\",\n",
    "                random_state=42,\n",
    "                n_init=\"auto\",\n",
    "            )\n",
    "            kmeans.fit(X)\n",
    "            y_kmeans = kmeans.predict(X) + cluster_offset\n",
    "\n",
    "            # Test whether the clusters found are acceptable\n",
    "            df_centroids_tmp = pd.DataFrame(y_kmeans, columns=[\"cluster\"])\n",
    "            gdf_centroids_tmp = bldng_centroids_dlg.join(df_centroids_tmp)\n",
    "\n",
    "            # compute the convex hull of a geoseries (of points); this is used to compute the convex hull of the cluster\n",
    "            # convex_hull on a geoseries is the geoseries of the convex hulls\n",
    "            # so we first have to union the points in our geoseries before computing the convex hull\n",
    "            def clustarea(x):\n",
    "                return x.unary_union.convex_hull.area\n",
    "\n",
    "            gdf_cluster_polygons = gdf_centroids_tmp.pivot_table(\n",
    "                index=\"cluster\", values=[\"geometry\"], aggfunc={\"geometry\": [clustarea]}\n",
    "            )\n",
    "            # Flatten multi level column index\n",
    "            gdf_cluster_polygons.columns = [\n",
    "                a[-1] for a in gdf_cluster_polygons.columns.to_flat_index()\n",
    "            ]\n",
    "            # Set constraints on the size of the clusters (to manage travel time within the cluster)\n",
    "            max_cluster_area = 1000000000\n",
    "            if mindist < 25:\n",
    "                max_cluster_area = math.pi * math.pow(\n",
    "                    1000, 2\n",
    "                )  # circle with 1000m radius\n",
    "            elif mindist < 50:\n",
    "                max_cluster_area = math.pi * math.pow(\n",
    "                    2500, 2\n",
    "                )  # circle with 2500m radius\n",
    "            else:\n",
    "                max_cluster_area = math.pi * math.pow(\n",
    "                    5000, 2\n",
    "                )  # circle with 5000m radius\n",
    "\n",
    "            n_too_large = len(\n",
    "                gdf_cluster_polygons.loc[\n",
    "                    gdf_cluster_polygons[\"clustarea\"] > max_cluster_area\n",
    "                ]\n",
    "            )\n",
    "            if n_too_large > 0:\n",
    "                # Increase number of clusters with 1\n",
    "                optimal_nr_clusters += 1\n",
    "                silhouette_coefficients = {}  # Reset silhouette coefficients\n",
    "                print(\n",
    "                    f\"{n_too_large} clusters have an area larger than ({round(max_cluster_area)}). Increasing # clusters to {optimal_nr_clusters}\"\n",
    "                )\n",
    "                n_clust_min = optimal_nr_clusters\n",
    "                n_clust_max = optimal_nr_clusters\n",
    "                repeat_dlg_clustering = True  # recluster\n",
    "            else:\n",
    "                repeat_dlg_clustering = False  # Stop iteration\n",
    "\n",
    "        # Clusters found for the DLG are ok, export\n",
    "        df_centroids_tmp = pd.DataFrame(y_kmeans, columns=[\"cluster\"])\n",
    "        gdf_centroids_tmp = bldng_centroids_dlg.join(df_centroids_tmp)\n",
    "        gdf_centroids_tmp.to_file(\n",
    "            f\"./data/{slugify(i_dlg_name)}_{slugify(i_gem_name)}/clusters_{slugify(i_dlg_name)}_{slugify(i_gem_name)}_optimal.shp\"\n",
    "        )\n",
    "\n",
    "        # Add centroids to global layer with labeled building centroids\n",
    "        # Added check if dataframes are empty to avoid deprecation warning for concat of empty dataframes\n",
    "        gdf_centroids = (\n",
    "            gdf_centroids_tmp\n",
    "            if gdf_centroids.empty\n",
    "            else pd.concat([gdf_centroids, gdf_centroids_tmp])\n",
    "        )\n",
    "\n",
    "        # Compute the classes for the centroids (which might be silly as they are probably the row index of kmeans.labels_, but can't find it in the documentation now. Better be safe.)\n",
    "        Xcentroids = np.column_stack(\n",
    "            (kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1])\n",
    "        )\n",
    "        y_centroids = kmeans.predict(Xcentroids)\n",
    "        # Create dataframe with the cluster centroids and the labels of the clusters\n",
    "        df_ccentroids_tmp = pd.DataFrame(\n",
    "            {\n",
    "                \"X\": kmeans.cluster_centers_[:, 0],\n",
    "                \"Y\": kmeans.cluster_centers_[:, 1],\n",
    "                \"cluster\": y_centroids + cluster_offset,\n",
    "            }\n",
    "        )\n",
    "        df_ccentroids_tmp[\"geometry\"] = df_ccentroids_tmp.apply(\n",
    "            lambda row: Point(row[\"X\"], row[\"Y\"]), axis=1\n",
    "        )\n",
    "        # todo: find better way to set crs\n",
    "        gdf_ccentroids_tmp = gpd.GeoDataFrame(df_ccentroids_tmp).set_crs(\n",
    "            crs=gdf_cnts.crs\n",
    "        )\n",
    "        gdf_ccentroids_tmp.to_file(\n",
    "            f\"./data/{slugify(i_dlg_name)}_{slugify(i_gem_name)}/clusters_{slugify(i_dlg_name)}_{slugify(i_gem_name)}_optimal_centroids.shp\"\n",
    "        )\n",
    "\n",
    "        # Add ccentroids to global layer with cluster centroids\n",
    "        # Added check if dataframes are empty to avoid deprecation warning for concat of empty dataframes\n",
    "        gdf_ccentroids = (\n",
    "            gdf_ccentroids_tmp\n",
    "            if gdf_ccentroids.empty\n",
    "            else pd.concat([gdf_ccentroids, gdf_ccentroids_tmp])\n",
    "        )\n",
    "\n",
    "        # Update cluster offset\n",
    "        cluster_offset += optimal_nr_clusters\n",
    "\n",
    "# Write all centroids to file\n",
    "gdf_centroids.to_file(f\"./data/clusters_optimal.shp\")\n",
    "# Write all ccentroids to file\n",
    "gdf_ccentroids.to_file(f\"./data/clusters_optimal_centroids.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
